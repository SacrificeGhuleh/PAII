// includes, cuda
#include <cstdint>
#include <climits>
#include <cuda_runtime.h>
#include <helper_cuda.h>

#include <cudaDefs.h>
#include <imageManager.h>

#include "arrayUtils.cuh"

#define BENCHMARK_NUM_REPS 100			// number of repetitions for benchmarking
#define TPB_1D 32						// ThreadsPerBlock in one dimension
#define TPB_2D 1024						// ThreadsPerBlock = 16*16 (2D block)
#define TPB_REDUCTION 512				// ThreadsPerBlock (1D block)

cudaError_t error = cudaSuccess;
cudaDeviceProp deviceProp = cudaDeviceProp();

using DT = uint8_t;						// Working data type

struct alignas(8) ResultType
{
	float fitness;
	uint32_t idx;
	
	ResultType& operator=(ResultType&&) = default;				//Forcing a move assignment operator to be generated by the compiler

	__host__ __device__ volatile ResultType& operator=(volatile const ResultType& other) volatile
	{
		fitness = other.fitness;
		idx = other.idx;
		return *this;
	}
};

struct Image
{
	uint32_t width = 0;
	uint32_t height = 0;
	uint32_t pitch = 0;
	DT* ptr = nullptr;
};

void prepareData(const char* imageFileName, Image& img)
{
	FIBITMAP* tmpA = ImageManager::GenericLoader(imageFileName, 0);
	img.width = FreeImage_GetWidth(tmpA);
	img.height = FreeImage_GetHeight(tmpA);
	img.pitch = FreeImage_GetPitch(tmpA);		// FREEIMAGE align row data ... You have to use pitch instead of width

	//Create a memory block using UNIFIED MEMORY to store original image. This is a redundant copy, however the data will be ready to use directly by GPU.
	uint8_t* tmpB = nullptr;
	size_t imageSize = static_cast<size_t>(img.pitch * img.height * FreeImage_GetBPP(tmpA)) >> 3;
	checkCudaErrors(cudaMallocManaged(&tmpB, imageSize));
	checkCudaErrors(cudaMemcpy(tmpB, FreeImage_GetBits(tmpA), imageSize, cudaMemcpyHostToDevice));
	//checkHostMatrix(tmpB, img.pitch, img.height, img.width, "%d ", "Reference");

	FreeImage_Unload(tmpA);

	//Create a memory block using UNIFIED MEMORY to store DT data and convert tmpB -> img.ptr
	checkCudaErrors(cudaMallocManaged(&img.ptr, img.width * img.height * sizeof(DT)));

	dim3 block{ 256,1,1 };
	dim3 grid{ getNumberOfParts(img.width * img.height, 256), 1, 1 };
	arrayReshape<uint8_t, DT> <<<grid, block>> > (tmpB, img.width, img.height, img.pitch, img.width, img.height, img.width*sizeof(DT), img.ptr);
	
	//From now, we have a new pitch of the final data.
	img.pitch = img.width * sizeof(DT);
	
	//Some synchronization must be called when using UNIFIED MEMORY in async. Previous kernel was called asynchronously!!!
	cudaDeviceSynchronize();
	//checkHostMatrix(img.ptr, img.width * sizeof(DT), img.height, img.width, "%0.2f ", "Reference");
}


//Every THREAD of 2D block [16x16] computes one final fitness value for a single pixel of the reference image. One corner of the query image is "virtually" attached to this pixel position. 
//A SINGLE THREAD compares the query image with the given region of the reference image. 
__global__ void find(const DT* __restrict__ ref, const uint32_t rWidth, const uint32_t rHeight,
			          const DT* __restrict__ query, const uint32_t qWidth, const uint32_t qHeight, 
					  ResultType* __restrict__ blockResults)
{
	uint32_t tid = threadIdx.x + threadIdx.y * blockDim.x;

	uint32_t rx = blockIdx.x * blockDim.x + threadIdx.x;
	uint32_t ry = blockIdx.y * blockDim.y + threadIdx.y;

	const uint32_t rxOffset = gridDim.x * blockDim.x;
	const uint32_t ryOffset = gridDim.y * blockDim.y;

	uint32_t qx, qy;

	const DT* r = nullptr;
	const DT* q = nullptr;

	__shared__ ResultType sData[TPB_2D];
	ResultType tmp;
			
	sData[tid] = { FLT_MAX, ry * rWidth + rx };

	while (ry <= rHeight-qHeight)							
	{
		rx = blockIdx.x * blockDim.x + threadIdx.x;

		while (rx <= rWidth - qWidth)						//It is supposed that we want to compare the whole pattern. It means that the query image must be completely inside the reference one.
		{
			tmp = {0.0f, ry * rWidth + rx };
			
			r = &ref[ry * rWidth + rx];						//Pointer to starting ROW position in the reference image.
			q = &query[0];									//Pointer to starting ROW position in the query image.

			for (qy=0; qy < qHeight; qy++)					//Each thread will process the whole query image		
			{
				for (qx = 0; qx < qWidth; qx++)				//Each thread will process the whole query image
				{
					tmp.fitness += (r[qx] - q[qx]) * (r[qx] - q[qx]);		//Cummulate the value
				}
				r += rWidth;								//Move one row down in the reference image.
				q += qWidth;								//Move one row down in the query image.
			}

			if (tmp.fitness < sData[tid].fitness)
			{
				sData[tid] = tmp;
			}
			rx+= rxOffset;									//Move to another pixel that will be the starting position for the comparison.
		}
		//Move down
		ry+= ryOffset;
	}

	__syncthreads();										//The parallel reduction will start here, all WARPS has to finish previous instructions.

	for (uint32_t s = (TPB_2D >> 1); s > 32; s >>= 1)		//This can be UNROLLED when the TPB is fixed for the application
	{
		if (tid < s)
		{
			if (sData[tid + s].fitness < sData[tid].fitness)
			{
				sData[tid] = sData[tid + s];
			}
		}
		__syncthreads();
	}
	if (tid < 32)											//Only one warm is active here, no sync is needed.
	{
		volatile ResultType* vsData = sData;
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 32].fitness) ? vsData[tid] : vsData[tid + 32];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 16].fitness) ? vsData[tid] : vsData[tid + 16];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 8].fitness) ? vsData[tid] : vsData[tid + 8];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 4].fitness) ? vsData[tid] : vsData[tid + 4];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 2].fitness) ? vsData[tid] : vsData[tid + 2];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 1].fitness) ? vsData[tid] : vsData[tid + 1];
	}

	if (tid == 0)											//0-th thread stores the final BEST result for a given block
	{
		blockResults[blockIdx.x] = sData[0];
	}

}

//One 1D block reduction
__global__ void getBest(ResultType* data, const uint32_t length)
{
	__shared__ ResultType sData[TPB_REDUCTION];
	
	uint32_t tid = threadIdx.x;
	const uint32_t offset = blockDim.x;
			
	sData[tid] = { FLT_MAX , tid };								//Initial fill of the shared memory

	if (tid < length)											
	{
		sData[tid] = data[tid];
	}

	uint32_t nextId = tid + offset;
	ResultType* ptr = &data[nextId];							//Pointer to global mem;

	while (nextId < length)										//Compare rest of data from the global memory
	{
		if (ptr->fitness < sData[tid].fitness)
		{
			sData[tid] = *ptr;
		}
		ptr += offset;
		nextId += offset;
	}
	__syncthreads();											//Start reduction from now

	for (uint32_t s = (TPB_REDUCTION >> 1); s > 32; s >>= 1)	//This can be UNROLLED when the TPB is fixed for the application
	{
		if (tid < s)
		{
			if (sData[tid + s].fitness < sData[tid].fitness)
			{
				sData[tid] = sData[tid + s];
			}
		}
		__syncthreads();
	}

	if (tid < 32)												//Only one warp is active here, no sync is needed.
	{
		volatile ResultType* vsData = sData;
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 32].fitness) ? vsData[tid] : vsData[tid + 32];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 16].fitness) ? vsData[tid] : vsData[tid + 16];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 8].fitness) ? vsData[tid] : vsData[tid + 8];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 4].fitness) ? vsData[tid] : vsData[tid + 4];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 2].fitness) ? vsData[tid] : vsData[tid + 2];
		vsData[tid] = (vsData[tid].fitness < vsData[tid + 1].fitness) ? vsData[tid] : vsData[tid + 1];
	}


	if (tid == 0)												//The zero thread saves the result into Global mem
	{
		data[0] = sData[0];
	}
}


int main(int argc, char* argv[])
{
	initializeCUDA(deviceProp);

	Image ref;
	Image query;

	FreeImage_Initialise();
	prepareData("./Data/reference.tif", ref);
	prepareData("./Data/query.tif", query);
	FreeImage_DeInitialise();

	cudaEvent_t start, stop;
	cudaEventCreate(&start);
	cudaEventCreate(&stop);

	//How many block of the size of [16x16] will process the reference image? 
	//Too much to manage. That's we use a 1D grid of [16x16] blocks that will move down the image.
	//This we need (((ref.width - query.width + 1) + 16 - 1)/16) blocks!!!
	uint32_t noBlocks = ((ref.width - query.width + 1) + TPB_1D - 1) / TPB_1D;

	ResultType* blockResults = nullptr;
	size_t blockResultsSize = static_cast<size_t>(noBlocks *sizeof(ResultType));
	checkCudaErrors(cudaMallocManaged(&blockResults, blockResultsSize));

	checkCudaErrors(cudaEventRecord(start, 0));

	//1. Try to compute all possible matches.
	dim3 block{ TPB_1D , TPB_1D ,1 };
	dim3 grid{ noBlocks, 1, 1 };
	find<<<grid, block>>>(ref.ptr, ref.width, ref.height, query.ptr, query.width, query.height, blockResults);

	//2. Search for the best match
	block = { TPB_REDUCTION ,1,1 };
	grid = { 1, 1, 1 };
	getBest<<<grid, block>>>(blockResults, noBlocks);
	cudaDeviceSynchronize();

	checkCudaErrors(cudaEventRecord(stop, 0));
	checkCudaErrors(cudaEventSynchronize(stop));
	float elapsedTime;
	checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, stop));
	cudaEventDestroy(start);
	cudaEventDestroy(stop);

	printf("Best fitness value: %f\n", blockResults[0].fitness);
	printf("Winner index: %u\n", blockResults[0].idx);
	printf("Winner's LEFT-TOP CORNER X: %u\n", blockResults[0].idx % ref.width);
	printf("Winner's LEFT-TOP CORNER Y: %u\n", ref.height - (blockResults[0].idx / ref.width) - query.height);
	printf("Computation time: %f ms\n", elapsedTime);

	if (ref.ptr) cudaFree(ref.ptr);
	if (query.ptr) cudaFree(query.ptr);
	if (blockResults) cudaFree(blockResults);
}
